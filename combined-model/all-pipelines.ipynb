{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 7\n",
    "# need to do the preprocessing for the data\n",
    "\n",
    "layer_dict['layer_7'] = dict()\n",
    "\n",
    "#label1 pipeline\n",
    "layer_dict['layer_7']['label_1'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=1000, gamma=0.001))\n",
    "])\n",
    "\n",
    "#label2 pipeline\n",
    "layer_dict['layer_7']['label_2'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=1000))\n",
    "])\n",
    "\n",
    "#label3 pipeline\n",
    "layer_dict['layer_7']['label_3'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC())\n",
    "])\n",
    "\n",
    "#label4 pipeline\n",
    "layer_dict['layer_7']['label_4'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=1000, class_weight='balanced'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 8\n",
    "# need to do the preprocessing for the data\n",
    "\n",
    "layer_dict['layer_8'] = dict()\n",
    "\n",
    "#label1 pipeline\n",
    "layer_dict['layer_8']['label_1'] = Pipeline([\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=100, gamma='scale'))\n",
    "])\n",
    "\n",
    "#label2 pipeline\n",
    "layer_dict['layer_8']['label_2'] = Pipeline([\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=30, gamma='scale'))\n",
    "])\n",
    "\n",
    "#label3 pipeline\n",
    "layer_dict['layer_8']['label_3'] = Pipeline([\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=100, gamma='scale'))\n",
    "])\n",
    "\n",
    "#label4 pipeline\n",
    "layer_dict['layer_8']['label_4'] = Pipeline([\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=30, gamma='scale'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 9\n",
    "# need to do the preprocessing for the data\n",
    "\n",
    "layer_dict['layer_9'] = dict()\n",
    "\n",
    "#label1 pipeline\n",
    "layer_dict['layer_9']['label_1'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('clf', LogisticRegression(C=100))\n",
    "])\n",
    "\n",
    "#label2 pipeline\n",
    "layer_dict['layer_9']['label_2'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('clf', KNeighborsClassifier(n_neighbors=1, p=2))\n",
    "])\n",
    "\n",
    "#label3 pipeline\n",
    "layer_dict['layer_9']['label_3'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('clf', SVC(C=100, kernel='rbf'))\n",
    "])\n",
    "\n",
    "#label4 pipeline\n",
    "layer_dict['layer_9']['label_4'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('clf', SVC(C=1, kernel='linear'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 10\n",
    "# need to do the preprocessing for the data\n",
    "\n",
    "layer_dict['layer_10'] = dict()\n",
    "\n",
    "#label1 pipeline\n",
    "layer_dict['layer_10']['label_1'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('clf', LogisticRegression(C=1))\n",
    "])\n",
    "\n",
    "#label2 pipeline\n",
    "layer_dict['layer_10']['label_2'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('clf', KNeighborsClassifier(n_neighbors=1, p=2))\n",
    "])\n",
    "\n",
    "#label3 pipeline\n",
    "layer_dict['layer_10']['label_3'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('clf', SVC(C=100, kernel='rbf'))\n",
    "])\n",
    "\n",
    "#label4 pipeline\n",
    "layer_dict['layer_10']['label_4'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('clf', SVC(C=100, kernel='linear'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 11\n",
    "# need to do the preprocessing for the data\n",
    "\n",
    "layer_dict['layer_11'] = dict()\n",
    "\n",
    "#same for all labels\n",
    "for label in ['label_1', 'label_2', 'label_3', 'label_4']:\n",
    "  layer_dict['layer_11'][label] = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', SVC(kernel='rbf', C=100, gamma='scale', random_state=42))\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 12\n",
    "# need to do the preprocessing for the data\n",
    "\n",
    "layer_dict['layer_12'] = dict()\n",
    "\n",
    "# label1 pipeline\n",
    "layer_dict['layer_12']['label_1'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=1500, gamma=0.001, kernel='rbf'))\n",
    "])\n",
    "\n",
    "# label2 pipeline\n",
    "layer_dict['layer_12']['label_2'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=100, gamma=0.001, kernel='rbf'))\n",
    "])\n",
    "\n",
    "# label3 pipeline\n",
    "layer_dict['layer_12']['label_3'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=100, gamma=0.001, kernel='rbf'))\n",
    "])\n",
    "\n",
    "# label4 pipeline\n",
    "layer_dict['layer_12']['label_4'] = Pipeline([\n",
    "  ('scaler', StandardScaler()),\n",
    "  ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "  ('clf', SVC(C=1000, gamma='auto', class_weight='balanced'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = []\n",
    "for i in range(1, 769):\n",
    "  FEATURES.append('feature_' + str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the data to the data_dict from the csv files\n",
    "for layer in ['layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12']:\n",
    "\n",
    "  data_dict[layer] = dict()\n",
    "\n",
    "  # read the data from the csv files\n",
    "  df_train = pd.read_csv('../Dataset/'+layer+'_train.csv')\n",
    "  df_valid = pd.read_csv('../Dataset/'+layer+'_valid.csv')\n",
    "  df_test = pd.read_csv('../Dataset/'+layer+'_test.csv')\n",
    "\n",
    "  # add the data to the data_dict\n",
    "  for label in ['label_1', 'label_2', 'label_3', 'label_4']:\n",
    "    data_dict[layer][label] = dict()\n",
    "\n",
    "    data_dict[layer][label]['x_train'] = df_train[df_train[label].notna()][FEATURES].values\n",
    "    data_dict[layer][label]['y_train'] = df_train[df_train[label].notna()][label].values\n",
    "    data_dict[layer][label]['x_valid'] = df_valid[df_valid[label].notna()][FEATURES].values\n",
    "    data_dict[layer][label]['y_valid'] = df_valid[df_valid[label].notna()][label].values\n",
    "    data_dict[layer][label]['x_test'] = df_test[df_test[label].notna()][FEATURES].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akila-i/.pyenv/versions/3.9.9/envs/ml-py3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/akila-i/.pyenv/versions/3.9.9/envs/ml-py3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# train pipelines\n",
    "for layer in layer_dict.keys():\n",
    "  for label in layer_dict[layer].keys():\n",
    "    layer_dict[layer][label].fit(data_dict[layer][label]['x_train'], data_dict[layer][label]['y_train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the train data (used in Meta Classifier)\n",
    "for layer in layer_dict.keys():\n",
    "  for label in layer_dict[layer].keys():\n",
    "    data_dict[layer][label]['y_train_pred'] = layer_dict[layer][label].predict(data_dict[layer][label]['x_train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrtie the predictions to the csv files\n",
    "for label in ['label_1', 'label_2', 'label_3', 'label_4']:\n",
    "  label_df = pd.DataFrame(columns=['layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12'])\n",
    "  for layer in ['layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12']:\n",
    "    label_df[layer] = data_dict[layer][label]['y_train_pred']\n",
    "  label_df.to_csv('../Outputs/'+label+'_train_pred.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Meta Model for label_1\n",
      "(28520,)\n",
      "Training Meta Model for label_2\n",
      "(28040,)\n",
      "Training Meta Model for label_3\n",
      "(28520,)\n",
      "Training Meta Model for label_4\n",
      "(28520,)\n"
     ]
    }
   ],
   "source": [
    "#import xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "meta_model_dict = dict()\n",
    "meta_model_data_dict = dict()\n",
    "df = pd.read_csv('../Dataset/layer_7_train.csv')\n",
    "\n",
    "for label in ['label_1', 'label_2', 'label_3', 'label_4']:\n",
    "  meta_model_dict[label] = SVC()\n",
    "  meta_model_data_dict[label] = dict()\n",
    "  meta_model_data_dict[label]['x_train'] = pd.read_csv('../Outputs/'+label+'_train_pred.csv')\n",
    "  meta_model_data_dict[label]['y_train'] = df[df[label].notna()][label].values\n",
    "\n",
    "# train the meta model for each label\n",
    "for label in meta_model_dict.keys():\n",
    "\n",
    "  print('Training Meta Model for', label)\n",
    "  # new y_train for meta model\n",
    "  # y_train_meta = data_dict['layer_7'][label]['y_train']\n",
    "  y_train_meta = meta_model_data_dict[label]['y_train']\n",
    "\n",
    "  print(y_train_meta.shape)\n",
    "  # print(pd.DataFrame(x_train_meta).head())\n",
    "  # print(pd.DataFrame(y_train_meta).head())\n",
    "  # train the meta model\n",
    "  meta_model_dict[label].fit(meta_model_data_dict[label]['x_train'], y_train_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = dict()\n",
    "\n",
    "# predict the labels for the valid data\n",
    "for label in ['label_1', 'label_2', 'label_3', 'label_4']:\n",
    "  valid_predictions[label] = dict()\n",
    "  for layer in layer_dict.keys():\n",
    "    valid_predictions[label][layer] = layer_dict[layer][label].predict(data_dict[layer][label]['x_valid'])\n",
    "\n",
    "# wrtie the predictions to the csv files\n",
    "for label in ['label_1', 'label_2', 'label_3', 'label_4']:\n",
    "  label_df = pd.DataFrame(columns=['layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12'])\n",
    "  for layer in ['layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12']:\n",
    "    label_df[layer] = valid_predictions[label][layer]\n",
    "  label_df.to_csv('../Outputs/'+label+'_valid_pred.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Classifier Accuracy for label label_1 : 0.8373333333333334\n",
      "Meta Classifier Accuracy for label label_2 : 0.8097826086956522\n",
      "Meta Classifier Accuracy for label label_3 : 0.9986666666666667\n",
      "Meta Classifier Accuracy for label label_4 : 0.8826666666666667\n"
     ]
    }
   ],
   "source": [
    "# predict the labels for the valid data (used in Meta Classifier)\n",
    "df = pd.read_csv('../Dataset/layer_7_valid.csv')\n",
    "for label in ['label_1', 'label_2', 'label_3', 'label_4']:\n",
    "  meta_model_data_dict[label]['x_valid'] = pd.read_csv('../Outputs/'+label+'_valid_pred.csv')\n",
    "  meta_model_data_dict[label]['y_valid'] = df[df[label].notna()][label].values\n",
    "\n",
    "  # predict the labels for the valid data\n",
    "  # valid_predictions[label]['meta'] = meta_model_dict[label].predict(x_valid_meta)\n",
    "  accuracy = meta_model_dict[label].score(meta_model_data_dict[label]['x_valid'], meta_model_data_dict[label]['y_valid'])\n",
    "  print('Meta Classifier Accuracy for label', label, ':', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pedictions = dict()\n",
    "\n",
    "# predict the labels for the test data (used in Meta Classifier)\n",
    "for layer in layer_dict.keys():\n",
    "  for label in layer_dict[layer].keys():\n",
    "    data_dict[layer][label]['y_test_pred'] = layer_dict[layer][label].predict(data_dict[layer][label]['x_test'])\n",
    "\n",
    "# wrtie the predictions to the csv files\n",
    "for label in ['label_1', 'label_2', 'label_3', 'label_4']:\n",
    "  label_df = pd.DataFrame(columns=['layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12'])\n",
    "  for layer in ['layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12']:\n",
    "    label_df[layer] = data_dict[layer][label]['y_test_pred']\n",
    "  label_df.to_csv('../Outputs/'+label+'_train_pred.csv', index=False)\n",
    "\n",
    "# predict from meta model\n",
    "for label in meta_model_dict.keys():\n",
    "  # new x_test for meta model\n",
    "  x_test_meta = []\n",
    "  for layer in layer_dict.keys():\n",
    "    x_test_meta.append(data_dict[layer][label]['y_test_pred'])\n",
    "  x_test_meta = np.array(x_test_meta).T\n",
    "\n",
    "  # predict the labels\n",
    "  final_pedictions[label] = meta_model_dict[label].predict(x_test_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
